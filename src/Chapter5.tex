\chapter{Extracting locations from Wifi data}
\label{locations}
% 20 pages
Human mobility has been attracting a high degree of attention from numerous
study fields among which we find urban and traffic planning, traffic prediction,
the spreading of diseases and many others \cite{AsgariGB13} \cite{Brockmann08}.

The studies that have been conducted on this subject have been using various
ways to identify the travel behaviour of people. Some of them have focused on
studying the information gathered from observing the way in which money is
dispersed through time \cite{Brockmann06}, or they have been focusing in
studying the behaviour of mobile phone users by analyzing the way they move
based on the communication towers their phones are connecting to when they are
engaging in voice communication \cite{Barabasi08}. There are studies that try to
understand human mobility through the glass of social networks
\cite{yang2010using}, as it can be observed that individuals prefer to meet with
other people that are part of their community more often
\cite{Musolesi:2007:DMM:1317425.1317433}. GPS data has also been considered for
various studies \cite{cuttone2014inferring}, \cite{5657695}. The list of
elements that have been taken into consideration for trying to understand and
predict the way in which we are conducting our daily travels is far from being
short. 

\section{Wifi based positioning}

Even from the beginning of the 21st century, research has been actively
conducted for trying to use the Wifi system in order to determine real
positioning and different databases for positioning systems have been created.
These databases usually included the positions of the Wifi access points or RF
(radio-frequency) identified fingerprints \cite{Chen:2006:PMP:2166283.2166297}
\cite{Cheng:2005:ACM:1067170.1067195} \cite{Youssef:2005:HWL:1067170.1067193}
\cite{bahl2000radar}. Modern databases for Wifi positioning are created with
information about the signal strength for the Wifi access points and can
even have information about where they were discovered.

Koo et. al. \cite{koo2011autonomous} have explored an algorithm that can help
estimate the relative positions of access points corresponding to the real
geographic configuration with the help of multidimensional scaling techniques.
Considering the fact that access points are not able to tell real distances
between themselves and other access points, the study aims to estimate the
dissimilarities between different access points using scans. They have also
conducted an experiment in an office building in order to test the proposed
algorithm and the results showed an estimation error of approximately $7$ m.

Another study conducted in this similar direction is the one by Mok et. al.
\cite{mok2007location}. The authors explore the possibility of determining the
location of a device which can scan Wifi access points based on the signal
strength that the access points are displaying at the moment of the scan. They
estimate the positioning by performing a trilateration based on the information
the device gets from multiple access points. The accuracy for their algorithm
for the conditions that were present in their experiment was of about $1-3$ m.

Athanasiou et. al. \cite{athanasiou2009utilizing} give a very clear and
concrete description for two classes of wireless positioning systems. Their work
focuses on experimenting with parameters for these algorithms in order to find
the optimal solution in terms of accuracy under realistic settings. They also
adapt a global map matching algorithm in order to extract travel time maps from
wireless data and they propose a demonstration for showing that for high
sampling frequencies, the locations identified are comparable to the ones
derived from GPS data.

The two classes of algorithms that are explored by the authors are: centroid and
fingerprinting. \textit{Centroid} is presented as the fastest method for
positioning, however it depends on having the real location of the access
points. This information is in general unavailable and as such a proposed
solution is to estimate the locations of the access points by calculating an
arithmetic mean of all the coordinates at which it was visible. The
\textit{fingerprinting} method is based on the assumption that the access points
are stable over time (they do not change positions). This leads to the fact that
at any time, a measurement at a particular location will return the same list of
access points with the same signal strengths. As such, this list can be
considered as the unique fingerprint of the location.

% TODO add 2 figures for centroid and fingerprinting
Zhang et.al. \cite{zhang2012polaris} propose an algorithm based on
fingerprinting for estimating locations that takes into consideration the fact
that the signal strength from various access points does not necessarily stay
constant throught the time. They propose a way in which a similarity between
fingerprints can be calculated in order to determine if two fingerprints are in
fact representing the same location.

These are just a selection of works that have been conducted on finding a
solution for Wifi based positioning systems. With the growth and improvement of
Wifi systems, in time all barriers can be overcome and we could have a
positioning system that is as accurate yet considerably cheaper than GPS
positioning systems.

\section{Determining the fingerprint of a location}
In order to have a better understanding about the way in which the mobile phone
users have been moving throughout the experiment, we needed to have an image of
the way a given period of time would look based on their Wifi records from
SensibleDTU. As it has been presented in Section~\ref{data_structures}, the Wifi
data we are using for the present project consists in the following fields:
user id, timestamp, SSID, BSSID, RSSI and the context. However, considering the
amount of data involved, just by looking through the log files it is almost
impossible for us to understand at what moment the user might have reached a
location and when did they leave from it. In order to be able to do this, we
have created various visualizations considering different options, different
time frames and for multiple users in order to begin to understand what the data
can tell us, what can we use, what would we need and what can we discard when
moving further to defining what makes a location.

\subsection{Signal strength over time}

The first thing that we have tried to visualize was the access points (APs) that
were scanned by users' mobile phones throughout different periods of time. We
have plotted the APs and their registered signal strength for varied users in
order to see if we notice any patterns in their movements.

%userX == user 6
In Fig.~\ref{user_6_1d_lines} we can see how a day from the life of a random
user (referred to as userX) looks like. The day for which we have plotted the
data started on a Tuesday at $12:15$ pm and ends the next day right before the
same hour. The hourly intervals can be seen on the X axis, while the signal
strength values can be seen on the Y axis. The legend contains the top $10$ most
popular~\footnote{An AP is more popular than another in case it appears more
times during the period of time for which the Wifi scans are analyzed}

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/user_6_sorted_1days_plot.png}
\caption{Example of the APs registered for an user throughout one day (using
connecting lines markers)}
\label{user_6_1d_lines}
\end{figure}

The steps for creating this type of visualization are as follows:
\begin{itemize}
  \item Retrieve data for the time duration for which the visualization is made
  \item Keep track of all the timestamps at which each AP has been seen and the
  AP's signal strength at that moment
  \item In case an AP is scanned no more than $2$ minutes after a it was
  previously scanned, then a line can unite the two moments in order to mark
  their proximity. If the apparitions are more than $2$ minutes apart there is
  a high possibility that there has been a location change or that the AP is
  experiencing technical problems and as such has stopped being active.
\end{itemize}

Although we have tried to visualize this type of information in various ways
(using different types of markers), we found that this way is the easiest to
interpret by people. If we leave out the lines, for example, as it can be seen
in Fig.~\ref{user_6_1d_point}, it is quite hard to interpret where location
might start or stop.

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/point_user_6_sorted_1days_plot.png}
\caption{Example of the APs registered for userX throughout one day (using
point markers)}
\label{user_6_1d_point}
\end{figure}

Other ways in which we have been experimenting with visualization for this can
be found in Appendix~\ref{appendix_signal_strength}.

By looking at Fig.~\ref{user_6_1d_lines} we are at some level able to
distinguish moments of time at which the user seems to be arriving at a
location~\footnote{For example, we can say that what we notice from Wednesday
at 10:15 until the same day at 12:15 is different than anything we can see
before that time so we can assume that it is a new location.}, however it is
hard to notice any patterns because we are only observing a single day in the
life of userX. 

% user Y = user 3
Let us look at the data gathered through $7$ days from another user's (referred
to as userY) life. The visualization for this data can be seen in
Fig.~\ref{user_3_7d}. The image gives out some very interesting information. We
can, for example, notice the repeating patterns which are dominated by the
orange, light green and blue colors. These patterns appear during the evening
and the night and we can assume that the user is spending this time at the
location which we can label ``home''.

We can notice some periods of time that are free. These free gaps like, for
example, from Monday morning until Monday evening are gaps in which no signal
was scanned and can mean that either the mobile phone was closed or that the
user decided to switch off the Wifi.

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/user_3_sorted_7days_plot.png}
\caption{Example of the APs registered for userY throughout 7 days}
\label{user_3_7d}
\end{figure}

We can also notice fragments in which the density of signals is quite high, for
example on Wednesday morning. This means that the user was located in a place
which has a large number of APs near and since we can notice a regularity in
this pattern we can assume that this place can be the University. This might
seem unlikely based on the fact that the patterns sometimes is identified during
the night, however this particular week is set in October when there are
deadlines for school projects that need to be handed in.

As we can see, these visualization can offer us a good first glance at what the
locations might be like, yet they also make us consider other things that we can
learn about the data. For example:
\begin{itemize}
  \item How many samples from each access point are received during a given time
  frame
  \item What is the average signal for various time frames for a given access
  point
  \item What are the running averages for signals from various access point
\end{itemize}
%As we can see, these visualization can offer us a good first glance at what the
%locations might be like, however, they also raise some interesting questions.
%For example:
%\begin{itemize}
%  \item Can the spikes in the signals cause any problems in determining
%  locations?
%  \item Does the sample density give us additional useful information for
%  extracting the locations?
%\end{itemize}

\subsection{Sample density}

When trying to identify locations based on the Wifi data, it is important to
only take into consideration the access points that actively contribute to the
fingerprint of the mentioned location. Before cleaning our data (as it has been
described in Section~\ref{data_cleaning}), isolated observable access points can
appear and unnecessarily burden the algorithm used for extracting the locations.
The best way to identify such access points is by analyzing the sample
density~\footnote{We define the sample density for an access point as the number
of times it appears in scans over a predefined time bin.} of the samples that
are identified during scanning.

In order to determine the sample density for each AP, we need to define a time
bin over which the sample density needs to be calculated. We have calculated the
density considering a time bin of $5$ minutes as we can assume that this amount
of time can be considered the minimum duration for which a user needs to be
situated in approximately the same place in order for us to not consider that
the location is a transition instead of a stop location.% TODO verify
% supposition

%TODO - userZ is 6 - 1 day , day 1
In Fig.~\ref{rssi_6_2nd_day} we have the different APs and their RSSI values at
the different moments when the mobile phone has identified them in the scans for
userX during the second day of observations. In Fig.~\ref{samples_6_2nd_day} we
can observe the sample density for one of the APs that are predominant during the visualized
time frame. As we can see, the number of times the AP is present in the scans
throughout the day is quite high and it is registered during numerous different
periods during the day. We can easily assume that this AP is one of the key APs
that define one of the locations the user has been associated with.

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/combinations/user_6_sorted_1days_plot_croped.png}
\caption{Example of the APs registered for userX throughout day 2}
\label{rssi_6_2nd_day}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/combinations/user_6_sorted_1days_plot_14280_histo.png}
\caption{Example of an AP which appears often}
\label{samples_6_2nd_day}
\end{figure}

On the opposite end as number of times it has appeared during the scans, we have
the AP in Fig.~\ref{few_samples_6_2nd_day}. As it can be seen, this AP only
appears $5$ times over a one single $5$ minute time bin. We can easily presume
that the presence or absence of this particular AP will not offer us relevant
information over the location at which the user was situated when it appeared in
the scans. This statement is also sustained by the fact that the user location
seems to be consisted from Wednesday $12:16$ up until around $13:16$ according
to what we can observe in Fig.~\ref{rssi_6_2nd_day}, even though the AP does not appear
throughout most of this time.

\begin{figure}[h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/combinations/1553_modif.png}
\caption{Example of an AP that appears just a few times}
\label{few_samples_6_2nd_day}
\end{figure}

Other examples of visualizations for APs based on their sample density can be
found in Appendix~\ref{appendix_sample_density}% TODO - add stuff to appendix
% TODO - modify and add fig with only one apparitions with 5 samples in 5
% minutes -> to modify and say we take into consideration only those with more
% than 5 samples in 5 minutes

\subsection{Exploring the implications of the signal strength}
Something that is often taken into consideration during studies regarding the
determination of locations based on Wifi data is the value which indicates
the signal strength received from the various APs. The level of the signal
strength indicator can, in general, give us a good approximation of how close we
are to a particular AP. However, Wifi networks are susceptible to interferences
\cite{MahantiCWA10}, meaning that there numerous factors which can cause signals
to spike even in case the device which scans the region for AP signals does not
move. This can represent a factor of risk when including the signal strength
value in the location extraction from Wifi data as the same location could be,
at different times, be associated to an AP which has a signal strength that
oscillates based on other external factors.

In order to see if we can smooth down possible fluctuations we have employed two
mathematical tools. We have calculated the average signal strength, as well as
the running average, considering different length time bins.

\subsection{Average signal strength}
\label{average_sig}

In order to calculate the average signal strength of a given AP for a given time
bin, we needed to identify all the moments of time inside the given time bin in which
the AP has been spotted during the scans. The average signal of the AP is
calculated as the sum of all the strength values that have been recorded for the
AP inside the time bin and the sum is then divided to the number of recorded
apparitions of the AP. For example, if we were to have an AP which appears 6
times inside a $5$ minutes time bin with the following RSSI values [-60, -70,
-60, -80, -90, -60], then the average signal strength for this particular time
bin for our AP would be $avg = [(-60) + (-70) + (-60) + (-80) + (-90) +
(-60)] / 6= -70$ dBm.

We have calculated the average signal for various users and various days. We
have also calculated it for different time bin length. For example, for the same
data that we can see in Fig.~\ref{rssi_6_2nd_day} and for the same AP that has
the sample density represented in Fig.~\ref{samples_6_2nd_day}, if we visualize
the non-null averages calculated for time bins of $5$ minutes, we would have the
representation in Fig.~\ref{user_6_avg_1d_5m}. The X axis records the time
while on the Y axis records the values of the averages

% TODO plot only with this AP in normal mode and maybe make some observations.
%TODO add more to appendix.

\begin{figure}[h]
\centering
\includegraphics[width =\textwidth]{figures/combinations/user_6_sorted_1days_plot_14280_avg_sig.png}
\caption{Example of average signal strength visualization for userX}
\label{user_6_avg_1d_5m}
\end{figure}

The averages are represented by big dots symbols which appear at the beginning
of the time bin for which the average is calculated. For example, if we have
calculated an average for the interval $12:05 - 12:10$, the average is plotted
on the visualization at $12:05$. 

Additional examples of averages for different APs scanned during the same day by
userZ's mobile phone can be found in Appendix~\ref{appendix_signal_strength}.

\subsection{Running average signal strength}
\label{running_avg}
The average signal brings a small improvement as far as eliminating the signal
spikes go, however, an even better way in order to smooth out any signal
fluctuations is to calculate the running average\footnote{Also referred to as
the moving average} \cite{Hyndman09movingaverages}.

We have calculated the running average for different users and time frames, and
we have taken into consideration different time bins when calculating it.
The algorithm for calculating it is as follows:
\begin{itemize}
  \item For the selected user and the selected time frame, we have extracted for
  each AP the time stamps at which it has been identified by the user's phone
  \item We have divided for each AP the previously mentioned time stamps into
  bins of $2$, $5$ or $10$ minutes recording also the signal
  strength identified at each time stamp\footnote{By doing this we have the
  signal strength for the given AP at any moments it has appeared inside the time bin}
  \item The above identified time bins are overlapping. For example, if a
  sequence of signals $[-60, -80, -70, -70]$ that have each been identified at $1$ minute
  apart is to be divided into bins of $2$ minutes, the resulting $2$
  minute bins would be: $[-60, -80]$, $[-80, -70]$, $[-70, -70]$
  \item The running average is calculated as the sum of the values present in a
  time bin which is then divided to the number of values. For example, for the
  above time bins, the running averages would be $-70$, $-75$ and $-70$
\end{itemize}

In Fig.~\ref{user_1_APs_1d} we can see the APs associated with another user
(referred to as userT) and their signal strengths over a day.
Fig.~\ref{user_1_AP85_1d} shows the signal strength for just one of the
identified APs. The average signal as is presented in Section~\ref{average_sig}
for the same AP can be seen in Fig.~\ref{user_1_AP85_avg_1d}.
Fig.~\ref{user_1_AP85_rn2avg_1d}, Fig.~\ref{user_1_AP85_rn5avg_1d} and
Fig.~\ref{user_1_AP85_rn10avg_1d} present the running averages calculated for
the same AP for time bins of $2$, $5$ and $10$ minutes~\footnote{In this
representation, only the non-null values for running averages are displayed}.
The X axis of these figures track the succession of time moments while the Y
axis keeps track of the value of the running average calculated over this time.

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_1days_plot.png}
\caption{Example of APs presence over time for userT}
\label{user_1_APs_1d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_85_plot.png}
\caption{AP 85 for userT during 1 day}
\label{user_1_AP85_1d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_1days_plot_85_avg_sig.png}
\caption{Average strength for AP 85 for userT during 1 day}
\label{user_1_AP85_avg_1d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_1days_plot_85_rn_avg_sig_2.png}
\caption{Running average for AP 85 for userT during 1 day (2 minute time bins)}
\label{user_1_AP85_rn2avg_1d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_1days_plot_85_rn_avg_sig_5.png}
\caption{Running average for AP 85 for userT during 1 day (5 minute time bins)}
\label{user_1_AP85_rn5avg_1d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth, height =
0.4\textwidth]{figures/rn_avg/user_1_sorted_1days_plot_85_rn_avg_sig_10.png}
\caption{Running average for AP 85 for userT during 1 day (10 minute time bins)}
\label{user_1_AP85_rn10avg_1d}
\end{figure}

The way in which the fluctuations are smoothed down can be easily seen in the
figures that present the running averages calculated for various time bins. The
fluctuations are smoother as the time bin is increased.

Visualizations for running averages calculated for other APs identified during
the same day for userT can be found in Appendix~\ref{appendix_rn_avg}.

\subsection{Signal presence}
\label{sig_pres}

Even though averaging the signal strength through time improves at a certain
level the fluctuations in the signal strength, in a real environment spikes will
always be present and this will bring extra difficulties in estimating locations
based on fingerprints that contain the value of the signal strength for the
involved APs.

Another way of looking at locations is by calculating their fingerprint based
only on the identity of the APs that have been identified while the user was
found at that particular location. Basically, instead of defining a location
based on both the identity of the APs present and their signal strength, we
would only associate locations to visible APs.

The idea is simple and elegant and has been used in previous studies with
success \cite{Larsen:2009:MCT:1813042.1813063}. The concept behind is that, in
general\footnote{New APs can be set up or old ones can be changed with new ones
in time, which would mean a change in how the scans would look for the same
location. However, this is an issue that is outside the scope of the present
paper and work.}, at a given location the scans will always show the presence of
the same APs. If, after a time, the scans change and other APs appear, it is
reasonable to assume that the user has changed locations.

Since the information offered by the signal strength does not seem to be of the
ultimate importance, we can, in this case, try to identify the locations only
based on the presence of the APs. We consider that an AP is present at a
specific moment of time if the Wifi scans at that moment register a signal
strength from that AP. However, as it has been mentioned previously, due to
interferences, the signal from the AP might be lost for short periods of time
even when the user does not change their location.Considering this and the
assumption that, in general, people tend to spend at least a few minutes in a
stop location (otherwise meaning that they might be just transiting it), we have
made the decision to adapt for our case the definition for the presence of an
AP.

We divide our data into time bins of $5$ minutes~\footnote{We consider 5 minutes
as the minimum amount of time that needs to be spent in a location for it be
considered a stop location. This number can be easily adjusted in case further
research shows that it is not the optimal assumption.}. We redefine the presence
of an AP as follows: an AP is considered to be present for the duration of a $5$
minute time bin if it appeared in the scans at any point inside this time
interval.

We can use visualization in order to see how this transforms the way in which we
can understand the data. In Fig.~\ref{user_6_APs_2d} we have the different APs
that have been scanned throughout the duration of $2$ days for userX. In
Fig.~\ref{user_6_pres_2d} we can see the top $50$ predominant APs and their
presence over $5$ minutes time bin during the same $2$ days~\footnote{We
restrict our visualization to $50$ APs as it would be hard to understand an
image in which we would be displaying all the hundreds of APs which were
encountered throughout the $2$ days.}. The X axis keeps track of the time bins
throughout the $2$ days, while the Y axis represents the annonymized identifiers
for the APs.

\begin{figure}[!h]
\centering
\includegraphics[width
=\textwidth]{figures/presence/user_6_sorted_2days_plot.png}
\caption{Scanned APs for userX throughout a duration of 2 days}
\label{user_6_APs_2d}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figures/presence/user_6_sorted_2days_no_rssi_plot.png}
\caption{The most common 50 APs for userX during the given 2 days (presence
visualization calculated for 5 minutes time bins)}
\label{user_6_pres_2d}
\end{figure}

By closely observing the two visualization, it is quite easy to see that indeed
they are representations of the same period of time. Even if not all APs are
displayed in the visualization for the presence of the APs over time, we can
notice that, for example, the user has spent the time from Wednesday $21:15$
until almost Thursday $6:15$ in one location. This also coincides with what we
can observe in the visualization for all the APs (with signal strength) scanned
throughout this time.

In Appendix~\ref{appendix_pres} can be found a visualization for the presence
of APs for a period of $2$ days for another user. The presence for APs is determined for $5$
minutes time bins over the $2$ days.

\section{Extracting locations}

By visualizing the Wifi data in the way presented in Section \ref{sig_pres}, we
can begin to see how locations seem to succeed each other throughout the days of
a particular user. However, it is important to be able to implement a solution
that will extract these locations from a large amount of data so that we would
not be needed to examine the data manually. We have used different methods in
order to get the best possible approximation for identifying the locations. The
methods we have tried are: using \textit{networks}, using \textit{k-means
clustering} and using \textit{Hidden Markov Models}.

Before describing each of these methods, we have to clarify what we consider a
fingerprint of a location at a given time. A fingerprint of a location is
calculated based on the AP presence inside $5$ minutes time bins
(Section~\ref{sig_pres}) as follows:
\begin{itemize}
  \item We extract the data we want to analyze from the user (either for 1, 2
  or more days).
  \item We identify the APs from the data
  \item We divide the data into 5 minute time bins
  \item For each time bin we identify the APs which have been spotted during the
  $5$ minutes and we attribute them the value of 1 (meaning that they are
  visible to the mobile device during that time bin); the remaining APs will
  have attributed the value 0 (not visible) for the given time bin.
  \item Each fingerprint describes a time bin and shows what APs were visible
  during it and which are not
\end{itemize}
for consecutive $5$ minutes time bins.
The fingerprint contains the names for all the APs which are associated to the user throughout the time
frame we are analyzing (e.g. 1 day, 5 days etc.) and each AP has associated to
it a value which represents its presence throughout the $5$ minutes 

\subsection{Network theory}

Networks have a high degree of importance when trying to understand human or
animal behaviour. They have been used in combination with social platforms in
order to extract a new definition of friendship \cite{cho2011friendship}, they
have been used for monitoring animal behaviour \cite{4116628}, or to understand
the economical situations cause by the way in which people interact
\cite{Copic05identifyingcommunity}, or just to understand underlying communities
of people. During our research, we have considered the use of network theory in
order to extract locations from Wifi data.

In theory, we can expect that the APs that are identified at a particular
location will not appear in the scans the user's phone will have from another
location. This assumption is sound as the APs will rarely be moved and as such
they should always be associated to the same place. In this case, we should
expect that the succession of locations can be similar to what we can see in
Fig.~\ref{user_6_tn}., where AP1-AP6 are associated to location number 1, while
the remaining APs are associated with location 2 and the APs never overlap.

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figures/networks/theoretical_network.png}
\caption{Example of how, in theory, locations should be displayed through the
presence of APs}
\label{user_6_tn}
\end{figure}

The idea behind constructing the network  that can be used to extract the
locations is simple. A graph can be created for each user from their data and
the locations can be identified as follows:
\begin{itemize}
  \item We consider each found AP from the user data as a node in the created
  graph
  \item We construct a presence matrix for the identified APs. Each line in the
  presence matrix is associated to an AP and contains the signal presence
  (Section \ref{sig_pres}) calculated for the AP considering $5$ minute time bins
  throughout the time we are evaluating
  \item For each time bin, we identify the APs that are present trough it and
  we connect each two of them with an undirected edge (in case they have not
  been connected at a previous time)
  \item Since signal from various APs can be lost due to interferences, for each
  two APs for which we have created a connecting edge, we keep and update a
  variable which represents the number of times the APs have been identified in
  the same time bin
  \item After the network is completely created, we normalize the counts of how
  many times each two APs have been seen in the same time bin by dividing the
  counted value to the maximum number of apparitions of either of the two access points
  \item After the normalization we remove the weak links~\footnote{In this
  case, a link is considered weak if after normalization its associated value is
  below a given threshold}
  \item We consider the resulting connected components to be the extracted
  locations
\end{itemize}

An example on how to construct such a graph if given four APs and their
presence matrix can be seen in Fig.~\ref{network_calc_example}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figures/networks/net_constr_ex.png}
\caption{Example of constructing a network}
\label{network_calc_example}
\end{figure}

We have applied the previously described algorithm for a selection of users, but
the results have not been satisfactory.

For example, we can take data for one day for userX. The visualization for the
identified APs and their presence throughout this time can be observed in
Fig.~\ref{user_6_pres_1d}. By looking at this image, we can observe that the
user has been in f$2$ main locations during this day.

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{figures/networks/user_6_sorted_1days_no_rssi_plot.png}
\caption{The most common 50 APs for userX during one day scan records}
\label{user_6_pres_1d}
\end{figure}

When running the algorithm that extract the locations based on the constructed
network, we obtain $21$ connected components which have the potential of being
locations. The image for the connected components can be seen in
Fig.~\ref{user_6_networks_1d} 

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/networks/user_6_day0_networks.png}
\caption{Locations identified with networks for userX during one day}
\label{user_6_networks_1d}
\end{figure}

We have tried to adjust the threshold for eliminating weak links yet the results
are in most cases unsatisfactory. Upon closer analysis we have observed that
this can be cause by the fact that, sometimes, there are a high number of APs
that even though they are in reality tied to a given location, their signal
fluctuates often and as such, the algorithm identifies them as part of a
different location and as such we have locations consisting in only a very small
number of APs that in reality could have been integrated in other locations.
This observation is sustained by the size distribution of the generated networks
(example for such a size distribution determined for the network in
Fig.~\ref{user_6_networks_1d} can be seen in
Fig.~\ref{user_6_size_distribution_1d}). Another thing we have observed is that
there are adjacent locations which can have interfering APs signals. This means
that our original supposition that, at all times, the APs that are visible from
a location will stop being visible in any other location does not always hold.

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/networks/communities-size-distribution.png}
\caption{Size distribution of locations based on the number of APs associated to
them }
\label{user_6_size_distribution_1d}
\end{figure}

%TODO cum ar arata efectiv locatiile fata de cum sunt
For generating the networks and the size distributions, we have been using
Gephi~\cite{Gephi}. For visualizing the visualization in
Fig.~\ref{user_6_networks_1d} we have used the Force Atlas $2$ layer which has
been configured in order to avoid overlapping of components.

\subsection{Cross validation}
\label{cross_valid}

For both the k-means and the Hidden Markove Models approaches on extracting
locations out of the user data we have been faced with a problem. The problem we
have been faced with is that both these algorithm need to know how many
locations they are trying to identify. However, we cannot know for sure, from
the beginning, how many locations a user has been visited during a given time.
In order for us to have a good estimation for the number of locations we could
be expecting to find inside a time frame, we have used the cross validation
technique, more specifically we have used the 10-fold cross validation method.

Cross validation \cite{Kohavi95astudy} is a technique for model validation that
tries to asses how the results given by a statistical analysis of some given
data can be generalized to an independent data set. The main use of cross
validation is in problems that deal with prediction. Prediction problems usually
deal with a set of training data and a set of testing data that the model needs
to be able to react to as expected.

The k-fold cross validation divides the data we have at our disposal in k equal
sized subsamples in a random way~\footnote{Random in this case means that
each of the subsamples contains elements from the original sample that are
most likly not in their original order.}. K-1 of the resulting subsamples are
used as training data, while the remaining subsample is used as testing data.
The samples are then rotated so as each of them becomes, in turn, testing
data while the others for the training data. The k results are then combined in
order to retrieve an unique estimation for the original data and an evaluation
of the accuracy of the prediction model can be done based on how close the
result is to the original data. The k value can be any number as long as the
data can be divided into k subsamples. A value that is oftenly used for k is 10
\cite{mclachlan2005analyzing}.

An example of how 2-fold cross validation works for four location fingerpints
can be seen in Fig.~\ref{2foldvalid}.

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{figures/kmeans/2-fold-validation.png}
\caption{Example for 2-fold cross validation}
\label{2foldvalid}
\end{figure}

In Step$1$ we have the four fingerprints. Let us consider that the algorithm
that we are using to extract the locations based on these fingerprints have
identified the location 1 and 2 as they can be seen in Step$1$. In order to see
if our algorithm behaved as expected, we can cross validate the result using in
this case a 2-fold cross validation. We are randomly selecting the $2$
subsamples as is seen in Step$2$. The blue color is associated with the training
data, while the orange color is associated with the testing data. In Step$3$ the
first subsample is treated as training data while in Step$4$ the second one
represents the training data. After the test data are classified based on the
training data we can combined the results into one single sample which is
presented in Step$5$. In this the cross validation has returned a result which
matches the original estimation made by our selected algorithm, which means that
the algorithm we have used has worked fine.

\subsection{K-means clustering}
\label{k-means}

The k-means algorithm is a popular method for analyzing clusters in data mining.
The first time the ``k-means'' term was used was by MacQueen
\cite{Macqueen67somemethods}, yet the standard algorithm for the k-means problem
was proposed by Lloyd \cite{Lloyd:2006:LSQ:2263356.2269955}. The idea behind
this algorithm is to start the clustering process by having k original groups of
only one point each. After this initial setup, each new point can be added to
the cluster that has the mean nearest to the new point. After a new point is
added to a group, the mean of the group is updated, and at each stage the k
means will represent the means of the k groups.

The Lloyd algorithm for k-means is the solution that is used for creating the
k-means tool by scikit-learn \cite{SL}. We have used the tool provided by
scikit-learn in order to try to extract the places where a user has been
situated at during a time frame based on the fingerprints that we can determine
for that time frame. Our goals was to use the k-means algorithm to cluster the
fingerprints that are similar enough as to be associated to the same location.
However, since we cannot know for sure the number of locations (clusters) we are
to expect for a given time frame, we are running the k-means algorithm with
different values for k and we perform 10-fold cross validation in order to see
what value has generate the most likely estimation.

The steps in extracting the locations with k-means and 10-fold cross validation
are as follow:
\begin{itemize}
  \item We select the time frame (number of days) for which we want to extract
  the locations
  \item We retrieve the data and extract the fingerprints
  \item Since previous research shows that in general people spend most of their
  time in a small amount of locations ($5$ to $50$) \cite{Barabasi08}, we choose
  the maximum number of locations we are expecting to find as the minimum value
  between $50$ and the rezult for the number of days multiplied by
  $10$\footnote{By observing the visualization for the presence of APs during
  different days in different users' life, we have observed that in general
  they seem to spend their time during a day in a most 10 locations}
  \item For each possible number for locations from between $2$ and our
  previously selected maximum we run the k-means clustering algorithm on the
  identified fingerprints
  \item The estimations are cross validated in order to see which number of
  locations has generated the optimal approximation for the given fingerprints
  \item In case more locations have generated equally good results we selected
  as number of expected locations the highest of them
  \item This algorithm is ran $10$ times leading to $10$ estimations for the
  number of locations. Out of these estimations the one which appears the most times
  out of the $10$ results is considered correct.
\end{itemize}

We ran the algorithm for $10$ times in order to ensure that we the final result
is as less influenced by the random fact involved in the determination of the
subsamples for the cross validation as possible. At the end of the algorith we
have the estimation for the locations throughout the selected time frame. An
example for such an estimation can be seen in Fig.~\ref{user_3_days1_2_kmeans},
while in Fig.~\ref{user_3_days1_2_APs_presence} we have the presence of the APs
which have been scanned throughout the same amount of time for the same user.

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{figures/kmeans/user_3_sorted_2days_no_rssi_plot.png}
\caption{The most common 50 APs for userY during the given 2 days (presence
visualization calculated for 5 minutes time bins)}
\label{user_3_days1_2_APs_presence}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.95\textwidth]{figures/kmeans/kmeans_locations_(6)_2days_plot.png}
\caption{Locations estimated with k-means for userY for 2 days}
\label{user_3_days1_2_kmeans}
\end{figure}

An additional example of locations estimated using k-means can be seen in
Appendix~\ref{appendix_kmeans}.

\subsection{Hidden Markov Models}
\label{hmm_section}

``Hidden Markov Models (HMMs) are a formal foundation for making probabilistic
models of linear sequence 'labeling' problems. They provide a conceptual
toolkit for building complex models just by drawing an intuitive picture. They
are at the heart of a diverse range of programs, including genefinding, profile
searches, multiple sequence alignment and regulatory site identification. HMMs
are the Legos of computational sequence analysis.''\cite{JOUR} 

HMMs are, in principle, Markov Models (MMs) \cite{opac-b1082850} for which the
modeled systems are considered to be processes with hidden states. If the the
MMs the states are visible to possible observers, the difference for the HMMs is
that the states are not visible, yet the results which can be observed do depend
on the hidden states \cite{18626}.

There area few elements that characterize the HMMs according to \cite{18626}.
They are as follows:
\begin{itemize}
  \item N which represents the number of hidden states in the model. In general,
  these states can be interconnected in a way so that from some states others
  can be reached
  \item M which represents the number of different observation symbols which
  care generated by the hidden states
  \item A which represents the state transition probability distribution. A is a
  matrix for which each element a$_{i,j}$ represents the probability of moving
  from the state i to the state j in the system represented by the HMM
  \item $B = {b_{j}(k)}$ which represents the observation system probability
  distribution in state j. This basically means that each element in B shows the
  probability of seeing a particular element of M in a given state j
  \item $\pi$ which represents the initial state distribution, meaning what is
  the probability of the system to start producing output from any of the states in
  N
\end{itemize}

The three problems also mentioned in \cite{18626} that the HMMs can be used for
to solve are as follows:
\begin{itemize}
  \item Given an observation sequence, how can the probability of the
  observation sequence be computed efficiently considering the given model
  \item Given the observation sequence how can a state sequence be chosen so
  that it explains in the most appropriate manner the existing observations
  \item How can the parameters of the model be adjusted in order to maximize the
  probability of a given observation sequence
\end{itemize}

The second of the three problems above addresses the uncovering of the hidden
states of a given model. Which is exactly what we are trying to identify when
attempting to extract what are the locations an user has been at based on
observing the APs that have been scanned throughout a given time frame for the
given user. In our case, N represents the number of unknown locations a user has
been at, M is the set of observable fingerprints which we can calculate based
on the presence of various APs in $5$ minutes time bins, and A, B and $\pi$ are
the various probability distributions that can be associated with the way in
which the user travels from location to location.

The idea of using HMMs in order to track localization is not new. It has been
explored in papers like \cite{el2013indoor}, \cite{inatomi2013hidden} or
\cite{morelli2007hidden} which sustain the potential of using an algorithm based
on this method of studying the travel behaviour of people.

Scikit-learn \cite{SL} offers an implementation for HMMs that ensures the
training for the models and the inferring of the hidden states and we have been
using the tools they provide for working with our data.

As with the k-means method (Section~\ref{k-means}), the problem we have been
facing was that we could not approximate from the beginning the number of hidden
states (which stand for locations in our case) that we are expecting the model
to find based on the input observations. However, by using the k-fold cross
validation (Section~\ref{cross_valid}) we can, once again (similar to the way in
which we have solved the problem for k-means), test the estimation being
computed based on different numbers of possible locations.

The steps in extracting the locations with the help of the HMM based algorithm
that has been combined with a 10-fold cross validation are as follow:
\begin{itemize}
  \item We select the time frame (number of days) for which we want to extract
  the locations
  \item We retrieve the data and extract the fingerprints
  \item We choose the maximum number of locations we are expecting to find in a
  similar manner we have done for the k-means algorithm, meaning as the minimum
  value between $50$ and the result for the number of days multiplied by $10$
  \item For each possible number of locations in the range of $2$ and our
  previously selected maximum we run the HMM algorithm on the existing
  fingerprints
  \item The estimations are cross validated in order to see which number of
  locations has generated the optimal approximation for the given fingerprints
  \item In case more locations have generated equally good results we select
  as number of expected locations the highest of them
  \item This algorithm is also ran $10$ times leading to $10$ estimations out of
  which the one which appears the most times out of the $10$ results is
  considered the correct one
\end{itemize}

The reason behind running the algorithm $10$ times is the same as the one
present for k-means algorithm. We want to ensure that the random factor which is
involved in the cross validation process has a very little effect on the
correctness of the estimation. At the end of the algorithm we have the hidden
states (in our case, locations) that can be extracted based on the observations
we have based on the presence of the various APs the user is associated to
throughout the given time frame. An example of locations that have been found
for userT throughout $1$ day can be seen in Fig.~\ref{user_6_days1_2_3_hmm}.
They can be easily mapped to the locations we can observe by looking at
Fig.~\ref{user_6_days1_2_3_APs_presence} where we have the presence of the
APs which have been scanned throughout the same amount of time for the same user.
%explain figures
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{figures/hmm/user_1_sorted_1days_no_rssi_plot.png}
\caption{The most common 50 APs for userT during the given day (presence
visualization calculated for 5 minutes time bins)}
\label{user_6_days1_2_3_APs_presence}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.95\textwidth]{figures/hmm/user_1_hmm_locations_(5)_1days_plot.png}
\caption{Locations estimated with HMM for userT for 1 day}
\label{user_6_days1_2_3_hmm}
\end{figure}

An additional example of locations estimated using the HMM based algorithm can
be seen in Appendix~\ref{appendix_hmm}.

%\begin{enumerate}
%  \item identifying locations
%	\begin{itemize}
%		\item Hidden Markov Models (5) 
%		\item K-means(2)
%	\end{itemize}
%  \item matching locations locations (0.5)
%	\begin{itemize}
%		\item Percentage similarity (0.5)
%		\item Keeping track of previous locations (0.5)
%		\item Creating fingerprints (1.5)
%	\end{itemize}
%  \item  
%\end{enumerate}